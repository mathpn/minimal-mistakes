---
title: "The basics of outlier detection"
header:
  overlay_image: /assets/images/default_overlay.jpg
  show_overlay_excerpt: false
  categories:
    - Data Analysis
  tags:
    - R
    - Statistics
excerpt: 'This post is intended to explain the basics of outlier detection and removal and, more specifically, to highlight some common mistakes.'
---

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Matheus Pedroni" />


<title>The basics of outlier detection</title>

<script src="/assets/outlier-basics/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="/assets/outlier-basics/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="/assets/outlier-basics/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="/assets/outlier-basics/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="/assets/outlier-basics/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="/assets/outlier-basics/navigation-1.1/tabsets.js"></script>
<link href="/assets/outlier-basics/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="/assets/outlier-basics/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">



<div id="outliers" class="section level2">
<h2>Outliers</h2>
<p>The word <em>outlier</em> is frequently used in my field of research (basic biology and biomedicine). It refers to data observations that differ a lot from the others. Some very informative posts regarding outliers are already available (see <a href="https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba">here</a> and <a href="https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561">here</a> for examples). However, my experience is that biology researchers often don’t receive adequate statistical education and rely on possibly dangerous heuristics to determine what is an outlier. This post is intended to explain the basics of outlier detection and removal and, more specifically, to highlight some common mistakes. Outliers may arise from experimental errors, human mistakes, flawed techniques (eg a batch of experiments done with low-quality reagent), corrupt data, or simply sampling probability.</p>
</div>
<div id="why-do-people-remove-outliers" class="section level2">
<h2>Why do people remove outliers?</h2>
<p>The question is valid: if data is obtained through standardized and reproducible procedures, why should valuable data points be tossed out? The answer is that standard statistical tests that rely on parametric assumptions are quite sensitive to outliers. This occurs mainly (but not exclusively) because the mean is very sensitive to extreme values (while the median is not, for example) - and the standard error is also sensitive in some way. So, as parametric tests usually rely solely on mean and variances to calculate the famous p-value, outliers often lead to weird results that do not seem plausible. For instance, let’s consider the data: 2.051501, 3.278150, 1.532082, 3.826658, 2.335235</p>
<p>A one-sample t-test tests if the mean is significantly different from zero. The result is that, as one would expect, it is indeed:</p>
<pre><code>##
##  One Sample t-test
##
## data:  c(2.051501, 3.27815, 1.532082, 3.826658, 2.335235)
## t = 6.2481, df = 4, p-value = 0.003345
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  1.447266 3.762184
## sample estimates:
## mean of x
##  2.604725</code></pre>
<p>However, let’s now add a point that makes the data <em>even farther from zero</em>: 2.051501, 3.278150, 1.532082, 3.826658, 2.335235, <strong>20</strong></p>
<pre><code>##
##  One Sample t-test
##
## data:  c(2.051501, 3.27815, 1.532082, 3.826658, 2.335235, 20)
## t = 1.8855, df = 5, p-value = 0.118
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -1.999914 13.007789
## sample estimates:
## mean of x
##  5.503938</code></pre>
<p>Well, now there’s no significant difference. The standard deviation of the sample increases a lot with the artificial outlier and so does the standard error of the mean, which is used to calculate confidence intervals and p-values. Thus, back in time when parametric statistics were all that there was and sample sizes were very limited, the solution was to remove (or change the value of) the outlying values.</p>
<p>Now, at this point it’s important to notice a few characteristics about outliers: by <strong>definition</strong>, outliers <strong>must be rare</strong>. If outliers are common (that is, more than a few percent of the observations at most), probably some data collection error has happened or the distribution is very far from a normal one. Another option is that the outlier detection method implemented <strong>detects too much of them</strong>.</p>
</div>
<div id="detecting-outliers" class="section level2">
<h2>Detecting outliers</h2>
<div id="z-scores" class="section level3">
<h3>Z-scores</h3>
<p>Z-scores are simply a way to standardize your data through rescaling. It shows how many standard deviations a given point, well, deviates from the mean. One can set an <strong>arbitrary threshold</strong> and exclude points that are above the positive threshold value or below the negative one. Let’s apply the z-score to our artificial data with a threshold value of 2:</p>
<pre class="r"><code>print(scale(c(2.051501, 3.27815, 1.532082, 3.826658, 2.335235, 20))[, 1])</code></pre>
<pre><code>## [1] -0.4828334 -0.3112829 -0.5554757 -0.2345725 -0.4431524  2.0273168</code></pre>
<p>This shows that our artificial outlier is indeed above the threshold and could be removed. However, many people consider <strong>2 a far too permissive threshold</strong> and use <strong>3 as a rule-of-thumb</strong> value. The outlier threshold will always be arbitrary and there’s no right answer, but using a low value can lead to frequent outlier detection - and <strong>outliers must be rare</strong>.</p>
<p>So, to sum it up, the z-score method is quite effective <strong>if the distribution of the data is roughly normal.</strong> The smaller the sample size, the more influence extreme values will have over the mean and the standard deviation. Thus, the z-score method <strong>may fail to detect extreme values in small sample sizes.</strong></p>
</div>
<div id="iqr-method" class="section level3">
<h3>IQR method</h3>
<p>The interquartile range (or IQR) method was created by the great statistician John Tukey and is embedded in the famous boxplot. The idea is to determine the 25th and 75th quantiles, that is, the values that leave 25% and 75% of the data below it, respectively. Then, the distance between them is the IQR. Below you can see a histogram of the famous height data from Sir Francis Galton in which those quantiles are marked with red lines. 50% of the data lies between the lines. <img src="/assets/outlier-basics/figure-html/unnamed-chunk-4-1.png" /><!-- --></p>
<p>The box-and-whisker plot (or boxplot) simply draws a box whose limits are the 25th and 75th percentiles, with the median (or 50th percentile) as a line in the middle. Then, whiskers of length 1.5 times IQR are drawn on both directions, that is, 1.5 times IQR below the 25th percentile and 1.5 times IQR above the 75th percentile. Values that are outside this range were considered outliers by Tukey. Here is the boxplot for the height data: <img src="/assets/outlier-basics/figure-html/fig1-1.png" style="display: block; margin: auto;" /></p>
<p>Some outliers do appear, but they are <em>very few</em>. The main advantage of the IQR method is that it’s more robust to slightly skewed distributions and <strong>it can detect outliers with smaller sample sizes</strong> as the median and IQR are much less influenced by extreme values than the mean and standard deviation, respectively. With z-scores, the presence of really extreme values can influence the mean and the standard deviation so much that it fails to detect other less extreme outliers, a phenomenon known as <strong>masking</strong>. In some cases, a factor of 2 or even 3 is used to multiply the IQR, detecting fewer outliers.</p>
<p>Using our previous artificial data, let’s replace the outlier with a less extreme value of 8 and apply both detection methods:</p>
<pre class="r"><code>print(scale(c(2.051501, 3.27815, 1.532082, 3.826658, 2.335235, 8))[, 1])</code></pre>
<pre><code>## [1] -0.61670998 -0.09587028 -0.83725721  0.13702825 -0.49623548  1.90904470</code></pre>
<p><img src="/assets/outlier-basics/figure-html/fig2-1.png" style="display: block; margin: auto;" /></p>
<p>The <strong>z-score</strong> method <strong>does not</strong> detect the extreme value as an outlier, while the IQR method does so. Let’s increase the sample size and repeat the analysis. The new data will be: 2.0515010, 3.2781500, 1.5320820, 3.8266580, 2.3352350, 3.4745626, 0.3231792, 3.3983499, 2.7515991, 4.5479615, 1.3167715, 1.8196742, 2.1908817, 1.8590404, 2.6546580, 3.5424431, 2.9777304, 2.6038048, 4.5722174, <strong>8</strong></p>
<pre class="r"><code>print(scale(c(2.0515010, 3.2781500, 1.5320820, 3.8266580, 2.3352350, 3.4745626, 0.3231792, 3.3983499, 2.7515991, 4.5479615, 1.3167715, 1.8196742, 2.1908817, 1.8590404, 2.6546580, 3.5424431, 2.9777304, 2.6038048, 4.5722174, 8))[, 1])</code></pre>
<pre><code>##  [1] -0.56445751  0.20373600 -0.88974560  0.54724118 -0.38676803
##  [6]  0.32674012 -1.64682548  0.27901164 -0.12601846  0.99896019
## [11] -1.02458460 -0.70963991 -0.47716983 -0.68498668 -0.18672819
## [16]  0.36925054  0.01559711 -0.21857520  1.01415054  3.16081217</code></pre>
<p>Now the artificial value is identified as an outlier, although the average of the other points is roughly the same. That is because the standard deviation and the mean get less sensitive to extreme values as the sample size increases. The IQR method also identifies the artificial point as an outlier in this case (graph not shown). The <strong>z-score</strong>, by definition, will <strong>never be greater than</strong> (n-1)/<span class="math inline">\(\sqrt{n})\)</span>, and that should be accounted for before analysis.</p>
</div>
<div id="mad-method" class="section level3">
<h3>MAD method</h3>
<p>The last method that we’ll cover is based on the median absolute deviation (MAD) and this method is often referred to as the <em>robust z-score</em>. It’s essentially a z-score, except the median will be used instead of the mean and the MAD instead of the standard deviation. The MAD is the median of the absolute values of the distance between each point and the sample median. In other words, it’s a standard deviation calculated with medians instead of averages. The new score, let’s call it the <strong>M-score</strong>, is given by: <span class="math display">\[M_i = \displaystyle \frac{0.6745(x_i - \tilde{x})}{MAD}\]</span> Where <span class="math inline">\(\tilde{x}\)</span> is the sample median and <span class="math inline">\(x_i\)</span> is each observation. Various thresholds have been suggested, ranging between 2 and 3. Let’s apply this method to our artificial outlier of 8 with a threshold of 2.24 as suggested before <span class="citation">(Iglewicz and Hoaglin 1993)</span>:</p>
<pre class="r"><code>m_data &lt;- c(2.051501, 3.27815, 1.532082, 3.826658, 2.335235, 8)
print(0.6745*(m_data - median(m_data)) / mad(m_data))</code></pre>
<pre><code>## [1] -0.3870867  0.2416539 -0.6533241  0.5228013 -0.2416539  2.6619214</code></pre>
<p>The artificial outlier is indeed above the threshold. The M-score suffers even less masking than the IQR and much less than the z-score. It’s robust to extreme outliers even with small sample sizes.</p>
</div>
</div>
<div id="the-dangers-of-outlier-removal" class="section level2">
<h2>The dangers of outlier removal</h2>
<div id="false-positives-type-i-error" class="section level3">
<h3>False positives (type I error)</h3>
<p>When performing exploratory data analysis, all outlier detection methods listed above are valid and each one has its pros and cons. They are useful to detect possible flaws in data collection but are also very useful to detect novelty and new trends. However, when performing inferential analysis, type I error rates (false positives) should be accounted for. Here, we’ll accept a 5% type I error rate, as usual. The graphic below shows the type I error rate for a two-sample Welch t-test drawing samples from a population of 10000 normally distributed points (mean = 0, sd = 1). For each sample sizes, 10000 t-tests are performed on independent samples.</p>
<p><img src="/assets/outlier-basics/figure-html/unnamed-chunk-8-1.png" /><!-- --></p>
<p>It’s quite alarming that, when using a threshold of <span class="math inline">\(|Z-score|\)</span> &gt; 2, the error rate goes up as the sample size increases. This shows that, although this is a very common threshold in published studies, it greatly inflates error rates and can be considered a form of <a href="https://en.wikipedia.org/wiki/Data_dredging"><em>p-hacking</em></a>. All methods resulted in error inflation, especially with smaller sample sizes. Let’s repeat this analysis using a population with skewed values:</p>
<pre class="r"><code>population &lt;- rgamma(10000, shape = 6)
hist(population)</code></pre>
<p><img src="/assets/outlier-basics/figure-html/unnamed-chunk-9-1.png" /><!-- --> <img src="/assets/outlier-basics/figure-html/unnamed-chunk-10-1.png" /><!-- --></p>
<p>The error inflation gets even <em>worse</em> when dealing with skewed distribution. Thus, <strong>caution should be taken</strong> before removing outliers with these methods as to <strong>whether the distribution is not heavily skewed</strong>.</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>Outlier detection and removal should be <strong>done with care</strong> and <strong>never without a well-defined method</strong>. Data that present distributions far from a normal one should not be subjected to the methods presented here. Removing outliers with <strong>small sample sizes</strong> (eg less than 20 observations per group or condition) can inflate type I error rates substantially and should be <strong>avoided</strong>. Outlier removal must be decided without taking into account statistical significance and the <strong>same method</strong> must be applied <strong>throughout the whole study</strong> (at least to similar data). If outliers are removed, they <strong>must be rare</strong> (as a rule-of-thumb, they must account for less than <em>5% of the data</em> - ideally less than 1%) and the method used to remove them as well as the number of observations removed <strong>must be clearly stated</strong>. Publishing the original data with outliers is also strongly advisable. Domain knowledge is key to determine when outliers are most likely due to error and not natural variability. Today, modern statistical techniques that are robust to extreme values exist and should be preferred whenever possible (for example, see the <a href="https://cran.r-project.org/web/packages/WRS2/vignettes/WRS2.pdf">WRS2 package</a>). Moreover, data that present non-normal distributions <strong>should not be forced into a normal-like distribution</strong> through outlier removal. The most important thing about outliers is to <strong>try to understand how they arise</strong> and to make efforts so that outliers don’t even appear - thus, rendering its removal unnecessary in most cases. In my experience, most outliers arise due to pre-analytical mistakes and small sample sizes. Thus, well-standardized techniques combined with parsimonious sample sizes can mitigate the issue in many cases.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-M_score_ref">
<p>Iglewicz, Boris, and 1944- Hoaglin David C. (David Caster). 1993. <em>How to Detect and Handle Outliers</em>. Book; Book/Illustrated. Milwaukee, Wis. : ASQC Quality Press.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
